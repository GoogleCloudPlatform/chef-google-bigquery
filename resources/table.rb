# Copyright 2018 Google Inc.
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# ----------------------------------------------------------------------------
#
#     ***     AUTO GENERATED CODE    ***    AUTO GENERATED CODE     ***
#
# ----------------------------------------------------------------------------
#
#     This file is automatically generated by Magic Modules and manual
#     changes will be clobbered when the file is regenerated.
#
#     Please read more about how to change this file in README.md and
#     CONTRIBUTING.md located at the root of this package.
#
# ----------------------------------------------------------------------------

# Add our google/ lib
$LOAD_PATH.unshift ::File.expand_path('../libraries', ::File.dirname(__FILE__))

require 'chef/resource'
require 'google/bigquery/network/delete'
require 'google/bigquery/network/get'
require 'google/bigquery/network/post'
require 'google/bigquery/network/put'
require 'google/bigquery/property/boolean'
require 'google/bigquery/property/enum'
require 'google/bigquery/property/integer'
require 'google/bigquery/property/namevalues'
require 'google/bigquery/property/string'
require 'google/bigquery/property/string_array'
require 'google/bigquery/property/table_bigtable_options'
require 'google/bigquery/property/table_column_families'
require 'google/bigquery/property/table_columns'
require 'google/bigquery/property/table_csv_options'
require 'google/bigquery/property/table_encryption_configuration'
require 'google/bigquery/property/table_external_data_configuration'
require 'google/bigquery/property/table_fields'
require 'google/bigquery/property/table_google_sheets_options'
require 'google/bigquery/property/table_schema'
require 'google/bigquery/property/table_streaming_buffer'
require 'google/bigquery/property/table_table_reference'
require 'google/bigquery/property/table_time_partitioning'
require 'google/bigquery/property/table_user_defined_function_resources'
require 'google/bigquery/property/table_view'
require 'google/hash_utils'

module Google
  module GBIGQUERY
    # A provider to manage Google Cloud BigQuery resources.
    class Table < Chef::Resource
      resource_name :gbigquery_table

      property :table_reference,
               [Hash, ::Google::Bigquery::Data::TableTableReference],
               coerce: ::Google::Bigquery::Property::TableTableReference.coerce, desired_state: true
      property :creation_time,
               Integer, coerce: ::Google::Bigquery::Property::Integer.coerce, desired_state: true
      property :description,
               String, coerce: ::Google::Bigquery::Property::String.coerce, desired_state: true
      property :friendly_name,
               String, coerce: ::Google::Bigquery::Property::String.coerce, desired_state: true
      property :id, String, coerce: ::Google::Bigquery::Property::String.coerce, desired_state: true
      property :labels,
               [Hash, ::Google::Bigquery::Property::NameValues],
               coerce: ::Google::Bigquery::Property::NameValues.coerce, desired_state: true
      property :last_modified_time,
               Integer, coerce: ::Google::Bigquery::Property::Integer.coerce, desired_state: true
      property :location,
               String, coerce: ::Google::Bigquery::Property::String.coerce, desired_state: true
      property :num_bytes,
               Integer, coerce: ::Google::Bigquery::Property::Integer.coerce, desired_state: true
      property :num_long_term_bytes,
               Integer, coerce: ::Google::Bigquery::Property::Integer.coerce, desired_state: true
      property :num_rows,
               Integer, coerce: ::Google::Bigquery::Property::Integer.coerce, desired_state: true
      property :type,
               equal_to: %w[TABLE VIEW EXTERNAL],
               coerce: ::Google::Bigquery::Property::Enum.coerce, desired_state: true
      property :view,
               [Hash, ::Google::Bigquery::Data::TableView],
               coerce: ::Google::Bigquery::Property::TableView.coerce, desired_state: true
      property :time_partitioning,
               [Hash, ::Google::Bigquery::Data::TableTimePartitioning],
               coerce: ::Google::Bigquery::Property::TableTimePartitioning.coerce,
               desired_state: true
      property :streaming_buffer,
               [Hash, ::Google::Bigquery::Data::TableStreamingBuffer],
               coerce: ::Google::Bigquery::Property::TableStreamingBuffer.coerce,
               desired_state: true
      property :schema,
               [Hash, ::Google::Bigquery::Data::TableSchema],
               coerce: ::Google::Bigquery::Property::TableSchema.coerce, desired_state: true
      property :encryption_configuration,
               [Hash, ::Google::Bigquery::Data::TableEncryptionConfiguration],
               coerce: ::Google::Bigquery::Property::TableEncryptionConfiguration.coerce,
               desired_state: true
      property :expiration_time,
               Integer, coerce: ::Google::Bigquery::Property::Integer.coerce, desired_state: true
      property :external_data_configuration,
               [Hash, ::Google::Bigquery::Data::TableExternalDataConfiguration],
               coerce: ::Google::Bigquery::Property::TableExternalDataConfiguration.coerce,
               desired_state: true
      property :dataset,
               String, coerce: ::Google::Bigquery::Property::String.coerce, desired_state: true

      property :credential, String, desired_state: false, required: true
      property :project, String, desired_state: false, required: true

      action :create do
        fetch = fetch_resource(@new_resource, self_link(@new_resource), 'bigquery#table')
        if fetch.nil?
          converge_by "Creating gbigquery_table[#{new_resource.name}]" do
            # TODO(nelsonjr): Show a list of variables to create
            # TODO(nelsonjr): Determine how to print green like update converge
            puts # making a newline until we find a better way TODO: find!
            compute_changes.each { |log| puts "    - #{log.strip}\n" }
            create_req = ::Google::Bigquery::Network::Post.new(
              collection(@new_resource), fetch_auth(@new_resource),
              'application/json', resource_to_request
            )
            return_if_object create_req.send, 'bigquery#table'
          end
        else
          @current_resource = @new_resource.clone
          @current_resource.table_reference =
            ::Google::Bigquery::Property::TableTableReference.api_parse(fetch['tableReference'])
          @current_resource.creation_time =
            ::Google::Bigquery::Property::Integer.api_parse(fetch['creationTime'])
          @current_resource.description =
            ::Google::Bigquery::Property::String.api_parse(fetch['description'])
          @current_resource.friendly_name =
            ::Google::Bigquery::Property::String.api_parse(fetch['friendlyName'])
          @current_resource.id = ::Google::Bigquery::Property::String.api_parse(fetch['id'])
          @current_resource.labels =
            ::Google::Bigquery::Property::NameValues.api_parse(fetch['labels'])
          @current_resource.last_modified_time =
            ::Google::Bigquery::Property::Integer.api_parse(fetch['lastModifiedTime'])
          @current_resource.location =
            ::Google::Bigquery::Property::String.api_parse(fetch['location'])
          @current_resource.num_bytes =
            ::Google::Bigquery::Property::Integer.api_parse(fetch['numBytes'])
          @current_resource.num_long_term_bytes =
            ::Google::Bigquery::Property::Integer.api_parse(fetch['numLongTermBytes'])
          @current_resource.num_rows =
            ::Google::Bigquery::Property::Integer.api_parse(fetch['numRows'])
          @current_resource.type = ::Google::Bigquery::Property::Enum.api_parse(fetch['type'])
          @current_resource.view =
            ::Google::Bigquery::Property::TableView.api_parse(fetch['view'])
          @current_resource.time_partitioning =
            ::Google::Bigquery::Property::TableTimePartitioning.api_parse(
              fetch['timePartitioning']
            )
          @current_resource.streaming_buffer =
            ::Google::Bigquery::Property::TableStreamingBuffer.api_parse(fetch['streamingBuffer'])
          @current_resource.schema =
            ::Google::Bigquery::Property::TableSchema.api_parse(fetch['schema'])
          @current_resource.encryption_configuration =
            ::Google::Bigquery::Property::TableEncryptionConfiguration.api_parse(
              fetch['encryptionConfiguration']
            )
          @current_resource.expiration_time =
            ::Google::Bigquery::Property::Integer.api_parse(fetch['expirationTime'])
          @current_resource.external_data_configuration =
            ::Google::Bigquery::Property::TableExternalDataConfiguration.api_parse(
              fetch['externalDataConfiguration']
            )

          update
        end
      end

      action :delete do
        fetch = fetch_resource(@new_resource, self_link(@new_resource), 'bigquery#table')
        unless fetch.nil?
          converge_by "Deleting gbigquery_table[#{new_resource.name}]" do
            delete_req = ::Google::Bigquery::Network::Delete.new(
              self_link(@new_resource), fetch_auth(@new_resource)
            )
            return_if_object delete_req.send, 'bigquery#table'
          end
        end
      end

      # TODO(nelsonjr): Add actions :manage and :modify

      private

      action_class do
        def resource_to_request
          request = {
            kind: 'bigquery#table',
            tableReference: new_resource.table_reference,
            description: new_resource.description,
            friendlyName: new_resource.friendly_name,
            labels: new_resource.labels,
            view: new_resource.view,
            timePartitioning: new_resource.time_partitioning,
            schema: new_resource.schema,
            encryptionConfiguration: new_resource.encryption_configuration,
            expirationTime: new_resource.expiration_time,
            externalDataConfiguration: new_resource.external_data_configuration
          }.reject { |_, v| v.nil? }
          request.to_json
        end

        def update
          converge_if_changed do |_vars|
            # TODO(nelsonjr): Determine how to print indented like upd converge
            # TODO(nelsonjr): Check w/ Chef... can we print this in red?
            puts # making a newline until we find a better way TODO: find!
            compute_changes.each { |log| puts "    - #{log.strip}\n" }
            update_req =
              ::Google::Bigquery::Network::Put.new(self_link(@new_resource),
                                                   fetch_auth(@new_resource),
                                                   'application/json',
                                                   resource_to_request)
            return_if_object update_req.send, 'bigquery#table'
          end
        end

        def self.resource_to_hash(resource)
          {
            project: resource.project,
            name: resource.name,
            kind: 'bigquery#table',
            table_reference: resource.table_reference,
            creation_time: resource.creation_time,
            description: resource.description,
            friendly_name: resource.friendly_name,
            id: resource.id,
            labels: resource.labels,
            last_modified_time: resource.last_modified_time,
            location: resource.location,
            num_bytes: resource.num_bytes,
            num_long_term_bytes: resource.num_long_term_bytes,
            num_rows: resource.num_rows,
            type: resource.type,
            view: resource.view,
            time_partitioning: resource.time_partitioning,
            streaming_buffer: resource.streaming_buffer,
            schema: resource.schema,
            encryption_configuration: resource.encryption_configuration,
            expiration_time: resource.expiration_time,
            external_data_configuration: resource.external_data_configuration,
            dataset: resource.dataset
          }.reject { |_, v| v.nil? }
        end

        # Copied from Chef > Provider > #converge_if_changed
        def compute_changes
          properties = @new_resource.class.state_properties.map(&:name)
          properties = properties.map(&:to_sym)
          if current_resource
            compute_changes_for_existing_resource properties
          else
            compute_changes_for_new_resource properties
          end
        end

        # Collect the list of modified properties
        def compute_changes_for_existing_resource(properties)
          specified_properties = properties.select do |property|
            @new_resource.property_is_set?(property)
          end
          modified = specified_properties.reject do |p|
            @new_resource.send(p) == current_resource.send(p)
          end

          generate_pretty_green_text(modified)
        end

        def generate_pretty_green_text(modified)
          property_size = modified.map(&:size).max
          modified.map! do |p|
            properties_str = if @new_resource.sensitive
                               '(suppressed sensitive property)'
                             else
                               [
                                 @new_resource.send(p).inspect,
                                 "(was #{current_resource.send(p).inspect})"
                               ].join(' ')
                             end
            "  set #{p.to_s.ljust(property_size)} to #{properties_str}"
          end
        end

        # Write down any properties we are setting.
        def compute_changes_for_new_resource(properties)
          property_size = properties.map(&:size).max
          properties.map do |property|
            default = ' (default value)' \
              unless @new_resource.property_is_set?(property)
            next if @new_resource.send(property).nil?
            properties_str = if @new_resource.sensitive
                               '(suppressed sensitive property)'
                             else
                               @new_resource.send(property).inspect
                             end
            ["  set #{property.to_s.ljust(property_size)}",
             "to #{properties_str}#{default}"].join(' ')
          end.compact
        end

        def fetch_auth(resource)
          self.class.fetch_auth(resource)
        end

        def self.fetch_auth(resource)
          resource.resources("gauth_credential[#{resource.credential}]")
                  .authorization
        end

        def fetch_resource(resource, self_link, kind)
          self.class.fetch_resource(resource, self_link, kind)
        end

        def debug(message)
          Chef::Log.debug(message)
        end

        def self.collection(data)
          URI.join(
            'https://www.googleapis.com/bigquery/v2/',
            expand_variables(
              'projects/{{project}}/datasets/{{dataset}}/tables',
              data
            )
          )
        end

        def collection(data)
          self.class.collection(data)
        end

        def self.self_link(data)
          URI.join(
            'https://www.googleapis.com/bigquery/v2/',
            expand_variables(
              'projects/{{project}}/datasets/{{dataset}}/tables/{{name}}',
              data
            )
          )
        end

        def self_link(data)
          self.class.self_link(data)
        end

        # rubocop:disable Metrics/CyclomaticComplexity
        def self.return_if_object(response, kind, allow_not_found = false)
          raise "Bad response: #{response.body}" \
            if response.is_a?(Net::HTTPBadRequest)
          raise "Bad response: #{response}" \
            unless response.is_a?(Net::HTTPResponse)
          return if response.is_a?(Net::HTTPNotFound) && allow_not_found 
          return if response.is_a?(Net::HTTPNoContent)
          result = JSON.parse(response.body)
          raise_if_errors result, %w[error errors], 'message'
          raise "Bad response: #{response}" unless response.is_a?(Net::HTTPOK)
          result
        end
        # rubocop:enable Metrics/CyclomaticComplexity

        def return_if_object(response, kind, allow_not_found = false)
          self.class.return_if_object(response, kind, allow_not_found)
        end

        def self.extract_variables(template)
          template.scan(/{{[^}]*}}/).map { |v| v.gsub(/{{([^}]*)}}/, '\1') }
                  .map(&:to_sym)
        end

        def self.expand_variables(template, var_data, extra_data = {})
          data = if var_data.class <= Hash
                   var_data.merge(extra_data)
                 else
                   resource_to_hash(var_data).merge(extra_data)
                 end
          extract_variables(template).each do |v|
            unless data.key?(v)
              raise "Missing variable :#{v} in #{data} on #{caller.join("\n")}}"
            end
            template.gsub!(/{{#{v}}}/, CGI.escape(data[v].to_s))
          end
          template
        end

        def self.fetch_resource(resource, self_link, kind)
          get_request = ::Google::Bigquery::Network::Get.new(
            self_link, fetch_auth(resource)
          )
          return_if_object get_request.send, kind, true
        end

        def self.raise_if_errors(response, err_path, msg_field)
          errors = ::Google::HashUtils.navigate(response, err_path)
          raise_error(errors, msg_field) unless errors.nil?
        end

        def self.raise_error(errors, msg_field)
          raise IOError, ['Operation failed:',
                          errors.map { |e| e[msg_field] }.join(', ')].join(' ')
        end
      end
    end
  end
end
